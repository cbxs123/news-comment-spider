ABSTRACT
Correlated topic modeling has been limited to small model and
problem sizes due to their high computational cost and poor scaling.
In this paper, we propose a new model which learns compact topic
embeddings and captures topic correlations through the closeness
between the topic vectors. Our method enables efcient inference
in the low-dimensional embedding space, reducing previous cubic
or quadratic time complexity to linear w.r.t the topic size. We
further speedup variational inference with a fast sampler to exploit
sparsity of topic occurrence. Extensive experiments show that our
approach is capable of handling model and data scales which are
several orders of magnitude larger than existing correlation results,
without sacrifcing modeling quality by providing competitive or
superior performance in document classifcation and retrieval.
CCS CONCEPTS
?Computing methodologies ¡ú Latent variable models;
KEYWORDS
Correlated topic models; topic embedding; scalability
1 INTRODUCTION
Large ever-growing document collections provide great opportuni-
ties, and pose compelling challenges, to infer rich semantic struc-
tures underlying the data for data management and utilization.
Topic models, particularly the Latent Dirichlet Allocation (LDA)
model[ 6 ], havebeenoneofthemostpopularstatisticalframeworks
to identify latent semantics from text corpora. One drawback of
LDA derives from the conjugate Dirichlet prior, as it models topic
occurrence (almost) independently and fails to capture rich topical
correlations (e.g., a document about virus may be likely to also be
about disease while unlikely to also be about fnance). Efective
modeling of the pervasive correlation paterns is essential for struc-
tural topic navigation, improved document representation, and
accurate prediction [ 5 , 9 , 37 ]. Correlated Topic Model (CTM) [ 5 ]
extends LDA using a logistic-normal prior which explicitly models
correlation paterns with a Gaussian covariance matrix.
Despite the enhanced expressiveness and resulting richer rep-
resentations, practical applications of correlated topic modeling
have unfortunately been limited due to high model complexity and